#!/usr/bin/env python3
"""
Test Qwen integration with Interview Copilot
"""

import os
import sys

def test_qwen_models():
    """Test different Qwen model options"""
    
    print("=== Qwen Model Options ===\n")
    
    # Available Qwen models in Ollama
    qwen_models = [
        "qwen2.5:0.5b",    # Very lightweight
        "qwen2.5:1.5b",    # Light
        "qwen2.5:3b",      # Medium
        "qwen2.5:7b",      # Recommended balance
        "qwen2.5:14b",     # More capable but heavier
        "qwen2.5:32b"      # Most capable but heavy
    ]
    
    print("Available Qwen models for Ollama:")
    for model in qwen_models:
        print(f"  ‚Ä¢ {model}")
    
    print("\nRecommended for Ryzen 5700U:")
    print("  ‚Ä¢ qwen2.5:3b  - Fastest, good enough")
    print("  ‚Ä¢ qwen2.5:7b  - Best balance (recommended)")
    print("  ‚Ä¢ qwen2.5:14b - Most capable if you have RAM")
    
    return qwen_models

def check_ollama_qwen():
    """Check if Qwen models are available in Ollama"""
    try:
        import requests
        
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            qwen_models = [m for m in models if 'qwen' in m['name'].lower()]
            
            print(f"\n‚úÖ Found {len(qwen_models)} Qwen models in Ollama:")
            for model in qwen_models:
                print(f"  ‚Ä¢ {model['name']} ({model['size']//1024//1024//1024}GB)")
            
            return True
        else:
            print("\n‚ùå Cannot connect to Ollama")
            return False
            
    except Exception as e:
        print(f"\n‚ùå Ollama connection error: {e}")
        return False

def pull_qwen_model(model_name: str = "qwen2.5:7b"):
    """Pull Qwen model from Ollama registry"""
    print(f"\nüì• Pulling {model_name}...")
    print("This may take several minutes depending on your internet speed.")
    print("Model will be cached locally for future use.\n")
    
    try:
        import subprocess
        result = subprocess.run(['ollama', 'pull', model_name], 
                              capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print("‚úÖ Model pulled successfully!")
            return True
        else:
            print(f"‚ùå Failed to pull model: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("‚è∞ Pull operation timed out. Try again later.")
        return False
    except Exception as e:
        print(f"‚ùå Error pulling model: {e}")
        return False

def test_qwen_performance():
    """Test Qwen response quality and speed"""
    print("\n=== Qwen Performance Test ===\n")
    
    try:
        from qwen_handler import QwenHandler
        
        # Test with local Ollama
        handler = QwenHandler(use_cloud_api=False)
        
        test_questions = [
            "–ß—Ç–æ —Ç–∞–∫–æ–µ React –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?",
            "–ö–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —É Next.js –ø–µ—Ä–µ–¥ –æ–±—ã—á–Ω—ã–º React?",
            "–û–±—ä—è—Å–Ω–∏—Ç–µ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É useState –∏ useEffect"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"–¢–µ—Å—Ç {i}: {question}")
            import time
            start_time = time.time()
            
            response = handler.generate_response(f"–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º: {question}")
            
            end_time = time.time()
            latency = end_time - start_time
            
            if response:
                print(f"‚úÖ –û—Ç–≤–µ—Ç ({latency:.1f}s): {response[:100]}...")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç")
            print()
            
    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")

def main():
    """Main test function"""
    print("Interview Copilot - Qwen Integration Test")
    print("=" * 50)
    
    # Check available models
    available_models = test_qwen_models()
    
    # Check Ollama connection
    if check_ollama_qwen():
        # Ask user which model to test
        print("\n–•–æ—á–µ—à—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å?")
        choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ 'skip' –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞: ").strip()
        
        if choice and choice != 'skip':
            if pull_qwen_model(choice):
                test_qwen_performance()
        else:
            print("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–∏.")
            test_qwen_performance()
    else:
        print("\nüîß –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Qwen:")
        print("1. –£—Å—Ç–∞–Ω–æ–≤–∏ Ollama: https://ollama.com/")
        print("2. –í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å: ollama pull qwen2.5:7b")
        print("3. –ó–∞–ø—É—Å—Ç–∏: python test_qwen_integration.py")

if __name__ == "__main__":
    main()